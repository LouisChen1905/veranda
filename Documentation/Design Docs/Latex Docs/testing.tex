% !TEX root = DesignDocument.tex


\chapter{System and Unit Testing Design}
This section describes the approach taken with regard to system and unit testing. It describes a number of specific tests and their results.  

\section{Overview}
The testing approach for this project is largely exploratory because the project is so graphically driven. These tests are basically comprised of simply observing the application after making changes, and determining whether or not those changes are reflected in the application.

There are also some unit tests for aspects of the project that can be tested algorithmically.

Much of our exploratory testing will be performed by beta users in the future.

\section{Test design and setup}
This section will describe how test cases were developed/designed, setup, and how they connect to the requirements.

\subsection{Application Window Opens Correctly}
Upon starting the program, a Qt window should open up in the foreground of the user's screen. This was tested through simple observation.

\subsection{Application Window Closes Correctly}
Upon clicking on the 'x' in the top right corner of the application window, it should be exited completely. This was tested through simple observation.

\subsection{User Interaction Generates Signals}
The user clicking buttons should generate signals which can cause events to be initiated by our code. In order to confirm that this works, slots were set up to listen to signals from the buttons which would cause visually obvious changes to occur within the application window. Then, these buttons were clicked, and it was confirmed that the correct changes happened.

\subsection{Application Window Resizes Correctly}
Resizing the window should be allowed, but should not cause graphical components to overlap, move outside the window, or shrink or grow to be unusually large or small on their x or y axis. This was tested by resizing the window first to be tall and thin, then short and thin, then short and wide, then tall and wide. It was confirmed that no unusual behaviors occured in any of these configurations.

\subsection{Robots Appear On Screen}
Each robot generated within the physics engine should also be visible within the simulation, assuming that the x and y coordinates of that robot are within the boundaries of the world view. To test this, a robot was generated at coordinates that should have put it in the center of the world view, and then it was confirmed that the world view now drew a circle at that location.

\subsection{Obstacles Appear On Screen}
Each obstacle generated within the physics engine should also be visible within the simulation, assuming that some part of the obstacle is within the boundaries of the world view. To test this, an obstacle was generated such that it was entirely within the world view, and then it was confirmed that the world view now drew the obstacle at that location.

\subsection{Application Receives Wheel Speeds}
The application should be able to receive wheel speeds from a seperate program, which it will use to apply forces to the robot within the physics simulation. To confirm that this work, we wrote a seperate Python application which publishes wheel velocities and observed our application using them to move a robot on the screen.

\subsection{Control Code Moves Robots}
Wheel speeds received from a separate program should move a robot in the active simulation accordingly. To test this, we set up the wheel speed generator to drive a robot in a figure-8. Then, with kinematic equations, these wheel speeds were converted to the velocity of the robot, which was handed to the physics simulation. The desired result was our simulated robot driving in a figure-8 after starting up the simulation. This was quite easy to evaluate visually.

This test was also run with the wheel speed generator outputting speeds to make the robot drive in a cricle, and again with speeds to make the robot drive back and forth along a line diagonally. Each drive pattern was observed working as intended.

\subsection{Collisions Function Correctly}
If a robot tries to move into or through another robot or an obstacle, it should fail to do so. Instead, it should stop just short, and its velocity in the direction of the thing it's colliding with should be set to zero, or it should push the thing it's colldigin with. Then, it can try again in the next time step. To test this, obstacles were set just inside each edge of the world view, so that the robot, spawned in the center of the world view would be boxed in. Then, the aforementioned figure-8 code was run and the robot was observed running into, but not entering each of the obstacles many, many times. 

In order to confirm that this would also work with other robots, three were all generated at different locations within the box of obstacles and were set to run with the same figure-8 control code. They were quickly observed colliding, and behaving in an acceptable manner.

\subsection{Multiple Robots Can Exists at Once}
Many robots should be able to exist at once, in different locations. To test this, three robots were loaded at different locations in the physics simulation, within the bounds of the world view, then it was confirmed that three circles appeared within the world view.

This fulfills the User Story contained in \ref{us:10}

\subsection{Changing Robot Properties in GUI Effects Simulation}
Changing the properties of a robot should have an immediate effect on the simulation. To test this, a property with a visually obvious result, specifically, the radius of the robot's touch sensor, was changed and it was observed that the size of touch sensor's indicator circle also changed.

\subsection{Touch Sensors Function Correctly}
Touch sensors should indicate when the robot they're attached to gets within a certain distance of another object in the world. To test this, a circle is being drawn to indicate a touch sensor. As the radius of the touch sensor is changed, the size of the circle can also be observed changing. Wherever an obstacle is detected by the touch sensor, it draws another much smaller circle along its radius. The robot was set to drive in a figure-8, and these circles were observed appearing and dissappearing as they should.

This contributes to the fulfillment of the User Story contained in \ref{us:7}

\subsection{Touch Sensors Publish ROS Messages}
Touch sensors should publish ROS messages to ROS tools whenever they're activated by a collision. This was tested by setting a robot with a touch sensor to drive in a figure-8 which was larger than the box of obstacles it was contained in. This caused many collisions, and therefore many touch sensor activations to occur. ROS messages were observed being generated properly by ROS tools.

\subsection{Selecting an Object in the Active Objects Menu Should Display the Properties of That Object}
When a row in the active objects menu that is not already highlighted is clicked by the user, it should become highlighted, and the properties being displayed in the Simulator Object Properties table should change to reflect the properties of the newly selected object. To test this, two robots with different touch sensor radius sizes were placed into the world, and it was observed that when the robots were switched between in the active objects menu, the touch sensor radius also changed in the simulator object properties table.

\subsection{When Switching Between Map and Designer Mode, the Map and Designer Mode Buttons Should Change Colors}
In map mode, the map mode button should be grey and the designer mode button should be blue. In designer mode, these colors should be swapped. To test this, the mode of operation switched back and forth from map mode to designer mode many times, and the colors were observed changing as intended.

\subsection{When Switching Between Map and Designer Mode, the Build Tools Should Change}
In map mode, the widget on the far right side of the screen should display map build tools, and in display mode, that widget should display designer build tools. To test this, the mode of operation was changed many times, and the widget was observed changing as intended.

\subsection{When Switching Between Map and Designer Mode, Available Buttons Should Change}
In map mode, there should be seven map design buttons below the mode of operation buttons, and in designer mode, there should be five designer buttons. To test this, the mode of operation was changed many times, and the buttons were observed changing as intended.

\subsection{Robot Designer Supports Multiple Body Shapes}
The user should be able to use the integrated robot designer to build robots with a number of body shapes, and to load them into the simulation. To test this, a robot was designed with a circular body, another with a rectangular body, and a third with an irregularly shaped body made up of connected polygons. Each was observed behaving as intended in the simulation while driving uninterrupted, and while colliding with other world objects.

This contributes to fulfillment of the User Story contained in \ref{us:3}

\subsection{Robot Designer Supports Multiple Drive Systems}
The user should be able to use the integrated robot designer to build robots with a number of drive systems, and to load them into the simulation. To test this, a robot was designed with differential drive and another with ackermann drive. Each was observed behaving as intended in the simulation while driving uninterrupted, and while colliding with other world objects.

This contributes to fulfillment of the User Story contained in \ref{us:3}

\subsection{Robot Designer Supports Multiple Sensors}
The user should be able to use the integrated robot designer to build robots with a number of sensors, and to load them into the simulation. To test this, a robot was designed with a lidar sensor and another with a touch sensor. Each was observed behaving as intended in the simulation while driving uninterrupted, and while colliding with other world objects.

This contributes to fulfillment of the User Story contained in \ref{us:3}

\subsection{Robot Designer Supports Any Number of Sensors on a Single Robot}
The user should be able to use the integrated robot designer to build robots with zero, one, or many sensors. Should they choose to put many sensors on a single robot, these sensors should be allowed to be of the same type, or differnet types. To test this, a robot was designed with no sensors, another with a single touch sensor, another with two lidar sensors, and another with one lidar and one touch sensor. Each was observed behaving as intended in the simulation while driving uninterrupted, and while colliding with other world objects. Each was also observed publishing messages as intended.

This contributes to fulfillment of the User Story contained in \ref{us:3}

\subsection{Integrated Joystick Tool Controls Robot}
The user should be able to use the integrated joystick tool to control a robot. To test this, the joystick tool was loaded up and connected to a robot, which was then controlled in the simulation by moving the joystick within the widget. Pushing the joystick towards the top of the widget was confirmed to cause the robot to move forwards, pushing the joystick towards the bottom was confirmed to make the robot move backwards, and pushing it towards either side was confirmed to make the robot turn left or right accordingly.

This fulfills the User Story contained in \ref{us:4}

\subsection{Drive System can be in Different Positions on Robot}
The user should be able to position a drive system wherever they want to on a robot. To test this, the robot designer was used to place a drive system in the middle of a robot, near the front of a different robot, and in the back left corner of another robot. Each was observed behaving as intended in the simulation while driving uninterrupted, and while colliding with other world objects.

This fulfills the User Story contained in \ref{us:5}

\subsection{Sensors can be in Different Positions on Robot}
The user should be able to position a sensor wherever they want to on a robot. To test this, the robot designer was used to place a sensor in the middle of a robot, near the front of a different robot, and to place one in the back left corner and back right corner of another robot. Each was observed behaving as intended in the simulation while driving uninterrupted, and while colliding with other world objects.

This fulfills the User Story contained in \ref{us:6}

\subsection{Lidar Sensors Visibly Detect Objects}
When an object is in range of a Lidar sensor, there should be a visual indication. This visual indication should be in the form of the lines representing the lidar sensor stopping short when they hit an object within their range. To test this, a robot with a lidar sensor was observed driving around many objects, and its sensor lines were observed shortening as intended while close enough to other objects.

This contributes to the fulfillment of the User Story contained in \ref{us:7}

\subsection{Images can be Imported as Maps}
The user should be able to load an image file as a map. This was tested by clicking the load map button and selecting an image file of a maze where all of the walls are completely black and everything else is completely white. The resulting map in the simulator was compared to the image, and confirmed to be very similar.

This contributes to the fulfillment of the User Story contained in \ref{us:9}

\subsection{User can Select Darkness Threshold for Image Map Importing}
The user should be able to select the darkness threshold for a map during the loading process. This should affect the objects created within the simulation. To test this, an image containing a rectangle that faded from completely white to completely black, surrounded by a white border on all sides was loaded at a threshold of 50, 100, 200, and 250. The size of the rectangle created in the simulation was observed to change accordingly.

This contributes to the fulfillment of the User Story contained in \ref{us:9}

\subsection{User can Select Size of Image Map in Simulator}
The user should be able to select the size of a map during the loading process. This was tested by loading the same image at different sizes and observing the objects loaded in the simulator to change sizes accordingly.

This contributes to the fulfillment of the User Story contained in \ref{us:9}

\subsection{Many Map Types can Be Loaded by Image Loader}
The map image loader should be able to load a wide variety of images as maps. This was tested by loading an image with rounded edges, an image with many colors in different shades, an image with concave shapes, and image with many seperate shapes, an image comprised of random noise, an entirely white image, and an entirely black image. Each was observed being loaded as expected.

This contributes to the fulfillment of the User Story contained in \ref{us:9}

\subsection{Maps can be Save and Loaded}
The user should be able to save and load maps as json files. These save states should include world objects, including robots and their properties. To test this, an image was loaded as a map, and two distinct robots were designed in the robot designer and placed into the simulation. The 'save map' button was clicked, and the json file format was selected. Then, a different image file was loaded as a map, two entirely different robots were designed and placed, and this too was saved. 

At this point, the first map was loaded by clicking the 'load map' button, selecting the json file format, and selecting the desired map. The simulation was observed getting set back to the state it had been in when the first map was saved. Robot objects and non-robot objects and all of their properties were maintained. Then the second map was loaded in the same way with the same results.

\subsection{Quicksave and Quickload Function Properly}
The user should be able to quicksave a simulator state, then, after making changes, return to that state with use of quickload. To test this, an image was loaded as a map, and two distinct robots were designed in the robot designer and placed into the simulation, then the quicksave button was clicked. Then, a different image file was loaded as a map, two entirely different robots were designed and placed.

Next, the quickload button was clicked and the simulation was observed getting set back to the state it had been in when the first map was saved. Robot objects and non-robot objects and all of their properties were maintained.

\section{Unit Testing}

\subsection{Dependencies}
We plan to use Catch unit testing framework for our unit testing. This only requires the inclusion of a single header and will be in the project; thus there are no outside dependencies.

\subsection{Writing and Running Tests}
Unit tests are included inside of the packages with the code that they test. The package \lstinline|pkg_CatchTesting| includes the Catch header file, a default Catch main source file, and a cmake file providing a macro to create Catch tests which can be run as part of the Ament build process.

\subsubsection*{Adding Tests}
Tests should be in their own C++ source file, which includes \lstinline|"Catch/catch.hpp"| which is available from the Catch Testing package. The source file should not define a \lstinline|main()| or invoke the Catch macro to create one; the file should contain only Catch tests.

Adding test files to the build is fairly simple, just copy the block in Listing \ref{lst:catchcmake} into the CMakeLists file for the package which the tests are for and modify it for the project. This block should be \textit{before} the \lstinline|ament_package()| function. Once tests are added to the build, they can be built and run using the command \lstinline|ament test|. This will run all unit tests and report whether they pass or fail. Unfortunately, the JUnit files exported by Catch are not compliant with GTest's modifications to the JUnit standard, so Ament will be unable to report which specific tests failed. Fortunately, the JUnit XML files can be found in \lstinline|[workspace root]\build\[package]\test_results\|; these files can be opened and read to determine which specific tests failed.

\begin{lstlisting}
if(BUILD_TESTING)
    find_package(sdsmt_simulator_catch_tests REQUIRED)

    ament_export_dependencies(
        sdsmt_simulator_catch_tests
    )

    # Hacky way to include the cmake file since (as far as I know) Ament does not provide a way to export
    # Other random files.
    set(CATCH_CMAKE "${sdsmt_simulator_catch_tests_INCLUDE_DIRS}/Catch/ament_cmake_add_catch_test.cmake")
    include(${CATCH_CMAKE})

    ament_add_catch_test(
        "[test name]"
        CPP_SOURCES [list of plain C++ sources]
        QT_SOURCES [list of C++ sources which need to be MOC'ed]
        QT_HEADERS [list of C++ headers which need to be MOC'ed]
        QT_LIBS [list of Qt Libraries to link against \{Core, Widgets, GUI, etc.\}]
        ROS_LIBS [list of other ROS package dependencies]
        LIBS [list of other libraries to link against]
        )

    # Example for building Model unit tests in the API package 
    # ament_add_catch_test(
    #     "model_datatype_tests"
    #     CPP_SOURCES tests/test_model.cpp
    #     QT_HEADERS include/sdsmt_simulator/model.h
    #     QT_LIBS Core
    #     ROS_LIBS "sdsmt_simulator_box2d"
    #     )
endif()
\end{lstlisting}

\subsection{List of Tests}
\subsubsection*{Tests for the Model class}
\begin{itemize}
    \item The Model transform is 0, 0, 0 on construction
    \item Using Model::setTransform changes the result of Model::getTransform correspondingly
    \item Setting Model transform results in Model::transformChanged being emitted
    \item Model shapes list is empty on construction
    \item Added shapes are reflected in calls to Model::shapes()
    \item Adding a shape multiple times does not create duplicates in Model::shapes()
    \item Removed shapes are reflected in calls to Model::shapes()
    \item Adding or removing one or more shapes results in Model::modelChanged being emitted (even if the shapes were added as duplicates or removed when they didn't exist)
    \item Model children list is empty on construction
    \item Added children are reflected in calls to Model::children()
    \item Adding a child multiple times does not create duplicates in Model::children()
    \item Removed children are reflected in calls to Model::children()
    \item Adding or removing one or more children results in Model::modelChanged being emitted (even if the models were added as duplicates or removed when they didn't exist)
    \item Calling Model::forceDraw results in Model::modelChanged being emitted
\end{itemize}
\subsubsection*{Tests for the Property class}
\begin{itemize}
    \item Testing of the static double\_validator function
    \item Testing of the static int\_validator function
    \item Testing of the static abs\_double\_validator function
    \item Testing of the static bool\_validator function
    \item Testing of the static angle\_validator function
    \item Property value can be set and retrieved with Property::set() and Property::get()
    \item Setting Property value results in emission of valueSet() in the Property and any PropertyValues observing it
    \item Requesting Property value from a PropertyView sets the value and results in emission of valueSet() in the Property and any PropertyValues observing it
    \item Requesting Property value from a PropertyView sets the value and results in emission of valueRequested() in the property
    \item Setting Property value can optionally emit valueRequested() in the Property
    \item PropertyValues can be copy-constructed to observe the same Property
    \item Deleting the Property observed by a PropertyView invalidates the PropertyView and does not cause nullptr dereferences
\end{itemize}